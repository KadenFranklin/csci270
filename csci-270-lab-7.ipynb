{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nfrom scipy.cluster import hierarchy\nimport os\nfrom IPython.display import display, Markdown\nfrom typing import *\n%matplotlib inline  ","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:35.035426Z","iopub.execute_input":"2022-03-04T13:39:35.036115Z","iopub.status.idle":"2022-03-04T13:39:35.685293Z","shell.execute_reply.started":"2022-03-04T13:39:35.035964Z","shell.execute_reply":"2022-03-04T13:39:35.684330Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Lab 7: Image Analysis\n\nIn this lab, you will:\n* Load images from files, representing them as arrays.\n* Shift images between different color representations.\n* Highlight intensity ranges via thresholding.\n* Extract a Region of Interest (ROI).\n* Create a histogram of image pixel values.\n* Use a color histogram to equalize image intensity levels.\n* Using a histogram of an ROI, filter the image to show regions sharing the ROI's color.\n* Find image features showing the relationship of an ROI to the original image.\n* Employ histograms for image clustering.\n* Employ feature extraction for image clustering.\n* Explore the advantages and disadvantages of these two approaches to image clustering.\n\nWe will be using the OpenCV Computer Vision library, imported as `cv2`. OpenCV uses `numpy` (`np`) arrays to represent images in memory. We will use `matplotlib` (which we previously used to show graphs) to display these `numpy` arrays as images in our notebook.","metadata":{}},{"cell_type":"markdown","source":"## Step 7.1: Setup\n\nFirst, we will load our images into memory from files. You should have access to the `CSCI 270 Images 2022` data set. If you have trouble accessing it, you can download the image files from the **Images** directory in the CSCI 270 Team, then upload them to Kaggle to create your own image data set.\n\nWe use the `cv2.imread()` function to read each image file into memory as a `numpy` array. The `images` list consists of tuples of each filename along with its `numpy` representation. We use the `show_markdown_table()` function to list all of the filenames, their list indices, and their images sizes.","metadata":{}},{"cell_type":"code","source":"def show_markdown_table(headers: List[str], data: List) -> str:\n    s = f\"| {' | '.join(headers)} |\\n| {' | '.join([(max(1, len(header) - 1)) * '-' + ':' for header in headers])} |\\n\"\n    for row in data:\n        s += f\"| {' | '.join([str(item) for item in row])} |\\n\"\n    display(Markdown(s))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:35.687099Z","iopub.execute_input":"2022-03-04T13:39:35.687414Z","iopub.status.idle":"2022-03-04T13:39:35.692664Z","shell.execute_reply.started":"2022-03-04T13:39:35.687373Z","shell.execute_reply":"2022-03-04T13:39:35.691966Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"prefix = '../input/csci-270-images-2022'\nimages = [(file, cv2.cvtColor(cv2.imread(f'{prefix}/{file}'), cv2.COLOR_BGR2RGB)) for file in os.listdir(prefix)]\nprint(f\"We have {len(images)} images.\")","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:35.693739Z","iopub.execute_input":"2022-03-04T13:39:35.694026Z","iopub.status.idle":"2022-03-04T13:39:38.746508Z","shell.execute_reply.started":"2022-03-04T13:39:35.693994Z","shell.execute_reply":"2022-03-04T13:39:38.745500Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"show_markdown_table(['Index', 'Filename', 'Width', 'Height'], [[i, file, img.shape[1], img.shape[0]] for (i, (file, img)) in enumerate(images)])","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:38.749077Z","iopub.execute_input":"2022-03-04T13:39:38.749698Z","iopub.status.idle":"2022-03-04T13:39:38.764376Z","shell.execute_reply.started":"2022-03-04T13:39:38.749647Z","shell.execute_reply":"2022-03-04T13:39:38.762753Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Find your image in the list. Then modify the definition of `my_image` in the code block below so that the `plt.imshow()` function displays your image. Currently, it shows the image listed above in row 0.","metadata":{}},{"cell_type":"code","source":"my_image = images[7][1]\nplt.imshow(my_image)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:38.766104Z","iopub.execute_input":"2022-03-04T13:39:38.766439Z","iopub.status.idle":"2022-03-04T13:39:39.440768Z","shell.execute_reply.started":"2022-03-04T13:39:38.766394Z","shell.execute_reply":"2022-03-04T13:39:39.440064Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"If you would like to adjust the size of your image, use the `figure` function to specify a larger size. Here is an example to make the above image larger. Adjust `figsize` to achieve different sizes.","metadata":{}},{"cell_type":"code","source":"plt.figure(num=None,figsize=(10,10),dpi=80)\nplt.imshow(my_image)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:39.442000Z","iopub.execute_input":"2022-03-04T13:39:39.442670Z","iopub.status.idle":"2022-03-04T13:39:40.186292Z","shell.execute_reply.started":"2022-03-04T13:39:39.442636Z","shell.execute_reply":"2022-03-04T13:39:40.185401Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# how to set 'region of interest'\nroi = images[7][1][200:450, 800:1100]\nplt.imshow(roi)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:40.187856Z","iopub.execute_input":"2022-03-04T13:39:40.188541Z","iopub.status.idle":"2022-03-04T13:39:40.436251Z","shell.execute_reply.started":"2022-03-04T13:39:40.188497Z","shell.execute_reply":"2022-03-04T13:39:40.435647Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Step 7.2: Color Representations and Conversions\n\nThere are many different representations of pixel colors. When you load your image, it is in BGR representation. Each pixel is represented as three values: a blue level, a green level, and a red level. The code block below shifts the blue to green, the green to red, and the red to blue. Run the block and see how your image changes.\n\nIt may take a while - loops like this in Python are really slow. We won't directly iterate over images very much - OpenCV operations are designed to be very efficient.","metadata":{}},{"cell_type":"code","source":"def color_shift_left(image):\n    shifted = image.copy()\n    for row in range(len(shifted)):\n        for col in range(len(shifted[row])):\n            blue = shifted[row][col][0]\n            shifted[row][col][0] = shifted[row][col][1]\n            shifted[row][col][1] = shifted[row][col][2]\n            shifted[row][col][2] = blue\n    return shifted\n\n%time plt.imshow(color_shift_left(my_image))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:40.437312Z","iopub.execute_input":"2022-03-04T13:39:40.438048Z","iopub.status.idle":"2022-03-04T13:39:55.119218Z","shell.execute_reply.started":"2022-03-04T13:39:40.438001Z","shell.execute_reply":"2022-03-04T13:39:55.118319Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Write a function called `color_shift_right()` that shifts green to blue, blue to red, and red to green. It will be very similar to `color_shift_left()`, except that the color rotation will be in the other direction.","metadata":{}},{"cell_type":"code","source":"def color_shift_right(image):\n    shifted = image.copy()\n    for row in range(len(shifted)):\n        for col in range(len(shifted[row])):\n            red = shifted[row][col][2]\n            shifted[row][col][2] = shifted[row][col][1]\n            shifted[row][col][1] = shifted[row][col][0]\n            shifted[row][col][0] = red\n    return shifted\n\n%time plt.imshow(color_shift_right(my_image))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:39:55.120516Z","iopub.execute_input":"2022-03-04T13:39:55.120743Z","iopub.status.idle":"2022-03-04T13:40:09.668826Z","shell.execute_reply.started":"2022-03-04T13:39:55.120716Z","shell.execute_reply":"2022-03-04T13:40:09.667799Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"To convert between color models, we use the `cv2.cvtColor()` function. It takes two arguments:\n* The image to be converted.\n* The conversion code, indicating the source and destination encoding.\n\nHere are some useful conversion codes:\n\n| Conversion       | Code                   | \n| ---------------- | ---------------------- |\n| BGR to grayscale | `cv2.COLOR_BGR2GRAY`   |\n| BGR to HSV       | `cv2.COLOR_BGR2HSV`    |\n| HSV to BGR       | `cv2.COLOR_HSV2BGR`    |\n| BGR to YCrCb     | `cv2.COLOR_BGR2YCrCb`  |\n| YCrCb to BGR     | `cv2.COLOR_YCR_CB2BGR` |\n\nWe will investigate HSV and YCrCb a little later. For now, we will focus on grayscale. The code block below converts to grayscale.","metadata":{}},{"cell_type":"code","source":"gray = cv2.cvtColor(my_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray, cmap='gray', vmin=0, vmax=255)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:09.672880Z","iopub.execute_input":"2022-03-04T13:40:09.673188Z","iopub.status.idle":"2022-03-04T13:40:10.319130Z","shell.execute_reply.started":"2022-03-04T13:40:09.673148Z","shell.execute_reply":"2022-03-04T13:40:10.318166Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Thresholding** an image turns all pixels in the threshold area white, leaving all other pixels black. Run the next two code blocks to see your image thresholded at two different levels.","metadata":{}},{"cell_type":"code","source":"ret, thresh = cv2.threshold(gray, 50, 255, 0)\nplt.imshow(thresh, cmap='gray', vmin=0, vmax=255)\nret","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:10.320443Z","iopub.execute_input":"2022-03-04T13:40:10.320673Z","iopub.status.idle":"2022-03-04T13:40:10.958000Z","shell.execute_reply.started":"2022-03-04T13:40:10.320645Z","shell.execute_reply":"2022-03-04T13:40:10.956959Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"ret, thresh = cv2.threshold(gray, 100, 255, 0)\nplt.imshow(thresh, cmap='gray', vmin=0, vmax=255)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:10.959879Z","iopub.execute_input":"2022-03-04T13:40:10.960200Z","iopub.status.idle":"2022-03-04T13:40:11.602864Z","shell.execute_reply.started":"2022-03-04T13:40:10.960157Z","shell.execute_reply":"2022-03-04T13:40:11.601969Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Step 7.3: Region of Interest and ORB Features\n\nTo obtain a Region of Interest (ROI), you can slice the `numpy` array storing the image.","metadata":{}},{"cell_type":"code","source":"peter = my_image[200:800, 800:1100]\nplt.imshow(peter)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:11.604211Z","iopub.execute_input":"2022-03-04T13:40:11.604516Z","iopub.status.idle":"2022-03-04T13:40:11.855975Z","shell.execute_reply.started":"2022-03-04T13:40:11.604478Z","shell.execute_reply":"2022-03-04T13:40:11.853591Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Examine your image. Select a region that is especially importand or distinctive. Then, in the code block below, create a slice that corresponds to your region of interest, and display it. Try different combinations of slice values until you have captured your ROI to your satisfaction.","metadata":{}},{"cell_type":"code","source":"my_roi = my_image[700:1150, 400:1000]\nplt.imshow(my_roi)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:11.857418Z","iopub.execute_input":"2022-03-04T13:40:11.858256Z","iopub.status.idle":"2022-03-04T13:40:12.165851Z","shell.execute_reply.started":"2022-03-04T13:40:11.858191Z","shell.execute_reply":"2022-03-04T13:40:12.164978Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"`ORB` is a feature-matching algorithm. It finds distinctive points (\"**keypoints**\") in an image, forms a model of the area around each of the points, and then finds the best matching corresponding points in another image. We will see if`ORB` can figure out where in your original image your ROI came from.\n\nWe will begin by using `ORB` to find the keypoints in the original image and display them. Adjust `figsize` as needed to make the image large enough to see the keypoints, which are highlighted in green.\n\nThe variable `kp` contains the coordinates of the keypoints. The variable `des` contains **descriptors**; that is, descriptions of the neighborhood around each keypoint.","metadata":{}},{"cell_type":"code","source":"orb = cv2.ORB_create()\nkp, des = orb.detectAndCompute(my_image, None)\nkeypoint_image = cv2.drawKeypoints(gray, kp, None, color=(255,0,0), flags=0)\nplt.figure(num=None,figsize=(10,10),dpi=80)\nplt.imshow(keypoint_image)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.167419Z","iopub.execute_input":"2022-03-04T13:40:12.167848Z","iopub.status.idle":"2022-03-04T13:40:12.974871Z","shell.execute_reply.started":"2022-03-04T13:40:12.167814Z","shell.execute_reply":"2022-03-04T13:40:12.974000Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Next, adapt the previous code block to find the keypoints in the ROI image `my_roi`. Store the keypoints in a variable named `kp_roi` and the descriptors in a variable named `des_roi`. Converting the ROI image to grayscale prior to viewing the keypoints will make them easier to see. Store the gray-converted roi image in a variable named `gray_roi`.","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE\norb = cv2.ORB_create()\nkp_roi, des_roi = orb.detectAndCompute(my_roi, None)\ngray_roi = cv2.drawKeypoints(gray, kp, None, color=(255,0,0), flags=0)\nplt.figure(num=None,figsize=(10,10),dpi=80)\nplt.imshow(keypoint_image)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:12.976252Z","iopub.execute_input":"2022-03-04T13:40:12.976462Z","iopub.status.idle":"2022-03-04T13:40:13.708544Z","shell.execute_reply.started":"2022-03-04T13:40:12.976436Z","shell.execute_reply":"2022-03-04T13:40:13.707824Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Now we will find the relationships between the keypoints in the two images. We will use a brute-force matcher that finds the best match between each descriptor in the first and second image. We will then draw the matches between the images. \n\nMatches are in terms of minimum distance, so sorting the matches will give us the closest matching descriptors first.\n\nThe match image produced should show lines connecting the 20 closest matching features. Hopefully, you will see matches between the ROI image and the part of the image from which it originally came.","metadata":{}},{"cell_type":"code","source":"brute_force = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\nmatches = sorted(brute_force.match(des, des_roi), key=lambda m: m.distance)\nmatch_image = cv2.drawMatches(my_image, kp, my_roi, kp_roi, matches[:20], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nplt.figure(num=None,figsize=(20,20),dpi=80)\nplt.imshow(match_image)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:13.709531Z","iopub.execute_input":"2022-03-04T13:40:13.710174Z","iopub.status.idle":"2022-03-04T13:40:14.937928Z","shell.execute_reply.started":"2022-03-04T13:40:13.710137Z","shell.execute_reply":"2022-03-04T13:40:14.935171Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Keypoint Questions\n\n1. How well did ORB match your ROI back to the original image?\n\n*answer*\n\n2. What aspects of your ROI do you think were helpful to ORB in performing the match?\n\n*answer*","metadata":{}},{"cell_type":"markdown","source":"## Step 7.4: Histograms\n\nAn image histogram records counts of the numbers of pixels of the given value. This means that for color images, there are separate histograms for each color channel. The code block below creates a histogram for each color channel, and displays all three histograms on a graph. Run the next two code blocks to see your image's color histograms. Then answer the questions in the box that follows.","metadata":{}},{"cell_type":"code","source":"def plot_histogram_of(image):\n    # Adapted from https://docs.opencv.org/4.x/d1/db7/tutorial_py_histogram_begins.html\n    color = ('r', 'g', 'b')\n    for i, c in enumerate(color):\n        hist = cv2.calcHist([image], [i], None, [256], [0, 256])\n        plt.plot(hist, color=c)\n        plt.xlim([0, 256])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:14.939049Z","iopub.execute_input":"2022-03-04T13:40:14.939310Z","iopub.status.idle":"2022-03-04T13:40:14.946030Z","shell.execute_reply.started":"2022-03-04T13:40:14.939262Z","shell.execute_reply":"2022-03-04T13:40:14.945031Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plot_histogram_of(my_image)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:14.947132Z","iopub.execute_input":"2022-03-04T13:40:14.947407Z","iopub.status.idle":"2022-03-04T13:40:15.150036Z","shell.execute_reply.started":"2022-03-04T13:40:14.947375Z","shell.execute_reply":"2022-03-04T13:40:15.149071Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Histogram Questions 1\n\n1. Which color is the most frequent in your image: Blue, Green, or Red?\n\n*answer*\n\n2. Which color is the least frequent?\n\n*answer*\n\n3. Are these frequencies consistent with how you intuitively understand the colors in the image? Explain your answer, in terms of what you see as the predominant colors (not just green, blue, and red). \n\n*answer*","metadata":{}},{"cell_type":"markdown","source":"Next, we will examine a histogram that splits out intensity (Y) from two color channels (Cr, Cb). The blue curve shows intensity while the red and green curves show the color channels.","metadata":{}},{"cell_type":"code","source":"plot_histogram_of(cv2.cvtColor(my_image, cv2.COLOR_BGR2YCrCb))","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:15.151647Z","iopub.execute_input":"2022-03-04T13:40:15.152050Z","iopub.status.idle":"2022-03-04T13:40:15.352352Z","shell.execute_reply.started":"2022-03-04T13:40:15.151926Z","shell.execute_reply":"2022-03-04T13:40:15.351302Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Histogram Questions 2\n\nHow well does the intensity level correspond with your intuitive idea about the relative brightness/darkness of your image?\n\n*answer*","metadata":{}},{"cell_type":"markdown","source":"We can use the intensity band from the `YCrCb` histogram to equalize the intensity levels of an image. To achieve this, we:\n* Convert the BGR image to YCrCb.\n* Get the Y values.\n* Use OpenCV's `equalizeHist()` function to equalize the Y levels.\n* Merge the equalized levels back with the CrCb values.\n* Convert the merged YCrCb image back to BGR and view.\n\nRun the code block below, then answer the questions that follow.","metadata":{}},{"cell_type":"code","source":"# Histogram equalization in color\n# Transition to YCrCb domain, then equalize on Y\n# Adapted from: https://stackoverflow.com/questions/42651595/histogram-equalization-python-for-colored-image\n\ny_cr_cb = cv2.cvtColor(my_image, cv2.COLOR_BGR2YCrCb)\ny, cr, cb = cv2.split(y_cr_cb)\ny_eq = cv2.equalizeHist(y)\ny_cr_cb_eq = cv2.merge((y_eq, cr, cb))\nplt.imshow(cv2.cvtColor(y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:15.353836Z","iopub.execute_input":"2022-03-04T13:40:15.354151Z","iopub.status.idle":"2022-03-04T13:40:16.025790Z","shell.execute_reply.started":"2022-03-04T13:40:15.354109Z","shell.execute_reply":"2022-03-04T13:40:16.025164Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Histogram Questions 3\n\n1. How drastic a change to your image does histogram equalization represent?\n\n*answer*\n\n2. Does the change increase or decrease the aesthetic appeal of your image?\n\n*answer*\n\n3. In general, under what circumstances do you consider this technique to be of greatest value?\n\n*answer*","metadata":{}},{"cell_type":"markdown","source":"We can also use a histogram to create a color filter we can use for thresholding. Select a region of your image that has a distinctive coloration pattern in the code block below.","metadata":{}},{"cell_type":"code","source":"# FILL IN SLICE COORDINATES BELOW\ncolor_roi = my_image[200:800, 800:1100]\nplt.imshow(color_roi)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:16.026721Z","iopub.execute_input":"2022-03-04T13:40:16.027500Z","iopub.status.idle":"2022-03-04T13:40:16.250432Z","shell.execute_reply.started":"2022-03-04T13:40:16.027461Z","shell.execute_reply":"2022-03-04T13:40:16.249735Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"We will use this ROI to create a **histogram backprojection** of the image. The backprojection has higher intensity levels to regions where the color is a good match to the histogram, and lower intensity levels to regions where the color is not as good a match.\n\nThis technique works best using an HSV (Hue, Saturation, Brightness) image, ignoring the Brightness component.\n\nRun the code block below and answer the questions that follow.","metadata":{}},{"cell_type":"code","source":"# https://docs.opencv.org/4.x/dc/df6/tutorial_py_histogram_backprojection.html\nimg_hsv = cv2.cvtColor(my_image, cv2.COLOR_BGR2HSV)\nsubimg_hsv = cv2.cvtColor(color_roi, cv2.COLOR_BGR2HSV)\n\nsubimg_hist = cv2.calcHist([subimg_hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])\ncv2.normalize(subimg_hist, subimg_hist, 0, 255, cv2.NORM_MINMAX)\nback = cv2.calcBackProject([img_hsv], [0, 1], subimg_hist, [0, 180, 0, 256], 1)\n\ndisc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\ncv2.filter2D(back, -1, disc, back)\n\nret, thresh = cv2.threshold(back, 50, 255, 0)\nthresh = cv2.merge((thresh, thresh, thresh))\nfiltered = cv2.bitwise_and(my_image, thresh)\n\nboth = np.vstack((thresh, filtered))\nplt.figure(figsize=(10,10))\nplt.imshow(both)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:16.251533Z","iopub.execute_input":"2022-03-04T13:40:16.251889Z","iopub.status.idle":"2022-03-04T13:40:17.399442Z","shell.execute_reply.started":"2022-03-04T13:40:16.251859Z","shell.execute_reply":"2022-03-04T13:40:17.398592Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Histogram Questions 4\n\n1. Look at the juxtaposed threshold image and filtered image. How well is your ROI's color represented in the final image?\n\n*answer*\n\n2. What are some practical applications you can envision of this technique?\n\n*answer*","metadata":{}},{"cell_type":"markdown","source":"## Step 7.5 Histogram Clustering\n\nWe can use color histograms to cluster the images in our data set. The next code block defines functions to display a dendrogram of the clustered images, using the Cosine distance to determine the clusters. The following code block creates histograms of each image in our data set. \n\nRun both code blocks and answer the questions that follow. Bear in mind that it may take a minute or two to run the clustering algorithm.","metadata":{}},{"cell_type":"code","source":"def dendrogram_from(names, distances, title):\n    condensed = distance.squareform(distances)\n    c = hierarchy.linkage(condensed, 'average')\n    plt.figure()\n    plt.figure(figsize=(10, 10))\n    dn = hierarchy.dendrogram(c, labels = names, leaf_rotation=90)\n    plt.title(f'UPGMA Clustering: {title}')\n    plt.ylabel(\"Total Distance\")\n    plt.show()\n    \ndef cluster_1d(names, images1d, title, distance_func):\n    distances = [[distance_func(a, b) for a in images1d] for b in images1d]\n    dendrogram_from(names, distances, title)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:17.400500Z","iopub.execute_input":"2022-03-04T13:40:17.400705Z","iopub.status.idle":"2022-03-04T13:40:17.408023Z","shell.execute_reply.started":"2022-03-04T13:40:17.400679Z","shell.execute_reply":"2022-03-04T13:40:17.407174Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"names = [i[0] for i in images]\nhistograms = [cv2.calcHist([i[1]], [0, 1, 2], None, [256] * 3, [0, 256] * 3).reshape(-1, 1) for i in images]\n%time cluster_1d(names, histograms, \"Histograms Cosine\", distance.cosine)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T13:40:17.409170Z","iopub.execute_input":"2022-03-04T13:40:17.409522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clustering Questions 1\n\n1. Select a few images that are marked as similar, and write some code blocks to view them. Which images did you choose? Intuitively speaking, do you find them to be similar?\n\n*answer*\n\n2. Select a few images that are marked as not similar, and write some code blocks to view them. Which images did you choose? Intuitively speaking, do you find them to be dissimilar?\n\n*answer*\n\n3. Overall, how well does the Cosine distance of histograms serve as a clustering technique? In other words, to what degree do the clusters match your intuition about the similarity of the images?\n\n*answer*","metadata":{}},{"cell_type":"markdown","source":"## Step 7.6 Feature Clustering\n\nWe can use `ORB` feature detection as a clustering technique as follows:\n* Find all the ORB keypoints and descriptors for all of our images.\n* Create a distance matrix as follows:\n  * For each pair of images:\n    * Find their keypoint matches, and sort them.\n    * Add up the top 20 distances of the closest matches. This will be the distance between the two images.\n* Call `dendrogram_from()` using that distance matrix.\n\nWrite the function `get_orb_matches()` to find the sorted ORB matches between two images. Then write `orb_distances()` to create the ORB distance matrix. (It should call `get_orb_matches()` as a subroutine.) Then run the following code block to view the resulting clusters.","metadata":{}},{"cell_type":"code","source":"def get_orb_matches(kp1, des1, kp2, des2):\n    #find all the sorted orb matches between two images\n    x = 1\n    \ndef orb_distances(images, num_matches):\n    kp_list = []\n    des_list = []\n    for img in images:\n        orb = cv2.ORB_create()\n        kp, des = orb.detectAndCompute(img, None)\n        kp_list.append(kp)\n        des_list.append(des)\n\n# i am so sorry, but i am going to use another token to do revisions on this lab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dendrogram_from(names, orb_distances(images, 20), \"ORB Distances (20 matches)\")","metadata":{"execution":{"iopub.status.busy":"2022-03-02T23:45:16.211116Z","iopub.execute_input":"2022-03-02T23:45:16.211375Z","iopub.status.idle":"2022-03-02T23:45:16.243128Z","shell.execute_reply.started":"2022-03-02T23:45:16.211348Z","shell.execute_reply":"2022-03-02T23:45:16.240766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clustering Questions 2\n\n1. Select a few images that are marked as similar, and write some code blocks using `get_orb_matches()` to view their keypoint correspondences. Which images did you choose? Intuitively speaking, do you find them to be similar?\n\n*answer*\n\n2. Select a few images that are marked as not similar, and write some code blocks to view them as in question 1. Which images did you choose? Intuitively speaking, do you find them to be dissimilar?\n\n*answer*\n\n3. Overall, how well does the ORB feature distance serve as a clustering technique? In other words, to what degree do the clusters match your intuition about the similarity of the images?\n\n*answer*\n\n4. Compare and contrast histogram distance and ORB feature distance as mechanisms for clustering. What advantages and disadvantages does each scheme have?\n\n*answer*","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}