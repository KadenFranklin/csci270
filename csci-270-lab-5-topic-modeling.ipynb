{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom typing import *\n\nimport os\nfrom matplotlib import pyplot as plt\nfrom IPython.display import display, Markdown\n\nfrom sklearn.decomposition import PCA\n\nimport gensim\nfrom gensim.models import word2vec as w2v\n\nfrom gensim.models import LdaModel\nfrom gensim.corpora import Dictionary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-18T15:48:51.396694Z","iopub.execute_input":"2022-02-18T15:48:51.397021Z","iopub.status.idle":"2022-02-18T15:48:51.403133Z","shell.execute_reply.started":"2022-02-18T15:48:51.396992Z","shell.execute_reply":"2022-02-18T15:48:51.402351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_markdown_table(headers: List[str], data: List) -> str:\n    s = f\"| {' | '.join(headers)} |\\n| {' | '.join([(max(1, len(header) - 1)) * '-' + ':' for header in headers])} |\\n\"\n    for row in data:\n        s += f\"| {' | '.join([str(item) for item in row])} |\\n\"\n    display(Markdown(s))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:48:51.407332Z","iopub.execute_input":"2022-02-18T15:48:51.407781Z","iopub.status.idle":"2022-02-18T15:48:51.422914Z","shell.execute_reply.started":"2022-02-18T15:48:51.40775Z","shell.execute_reply":"2022-02-18T15:48:51.421854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Latent Dirichlet Allocation (LDA)\n\nA **topic** is a set of terms that suggest a shared theme. You'll use **Latent Dirichlet Allocation (LDA)** to generate five topics for your book. \n","metadata":{}},{"cell_type":"code","source":"# Replace the argument with the filename of your book.\nsentences = w2v.LineSentence('../input/csci-270-tokenized-books-2022/huck-finn_fixed.txt')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:48:51.424615Z","iopub.execute_input":"2022-02-18T15:48:51.425074Z","iopub.status.idle":"2022-02-18T15:48:51.435852Z","shell.execute_reply.started":"2022-02-18T15:48:51.425041Z","shell.execute_reply":"2022-02-18T15:48:51.435112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running LDA\n\nCreate an LDA topic model for your book using the model in the `sentences` variable defined above. Run the `lda_analysis()` function below for this purpose. It will generate 5 topics for you to examine, each containing 20 words. It may take a couple of minutes to complete.","metadata":{}},{"cell_type":"code","source":"def lda_analysis(sentences):\n    dictionary = Dictionary(sentences)\n    dictionary.filter_extremes(no_below=1, no_above=0.8)\n    corpus = [dictionary.doc2bow(text) for text in sentences]\n    lda = LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=10000, passes=10)\n    return lda.show_topics(formatted=False, num_words=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:48:51.437294Z","iopub.execute_input":"2022-02-18T15:48:51.437698Z","iopub.status.idle":"2022-02-18T15:48:51.450411Z","shell.execute_reply.started":"2022-02-18T15:48:51.437648Z","shell.execute_reply":"2022-02-18T15:48:51.449487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time lda_analysis(sentences)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:48:51.530659Z","iopub.execute_input":"2022-02-18T15:48:51.530921Z","iopub.status.idle":"2022-02-18T15:49:59.198313Z","shell.execute_reply.started":"2022-02-18T15:48:51.530893Z","shell.execute_reply":"2022-02-18T15:49:59.197378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing Topics\n\nExamine the five topics generated by the function call above. Then answer the following questions:\n\n1. For each topic, can you perceive a coherent aspect of your book that summarizes the terms included in the topic? If so, state that aspect and justify it. If not, speculate as to how that collection of terms is in some way representative of the book.\n\n**I am having a hard time with this one. I have rerun the function several times to try and disect different topics. They all seem to be indiscernable from a single topic. This is probably because a majority of the story is told through narration or other stories that characters retell.**\n\n2. What themes or important concepts from the book were not represented in the topics that arguably could have been? Don't focus on specific terms; instead, think about abstractions that transcend individual terms.\n\n**In the book there is a significant amount of lying, stealing, pretending/impersonating, and just general mischief. I expected this to be represented in some of the topics, but they do not seem to be.**\n\n3. What terms in the topic represent vocabulary specific to the book, as opposed to more generic terms? Examples include major characters, locations, objects, etc.\n\n**There are several terms specific to the dialect some characters speak, because of the the setting and time period of the book. I have chosen not to include them because they are just different forms of more generic terms. Other terms:\n    Tom, huck, jim, king, duke, mary, river**\n    \n4. What terms from the book were **not** represented in the topics that arguably could have been? Examples again include major characters, locations, objects, etc.\n\n**raft, father, lie, steal, hide**","metadata":{}},{"cell_type":"markdown","source":"## Additional books\n\nSelect two other books from our corpus, and run topic analysis on each one. ","metadata":{}},{"cell_type":"code","source":"%time lda_analysis(w2v.LineSentence('../input/csci-270-tokenized-books-2022/KJV_Bible.txt'))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:49:59.200096Z","iopub.execute_input":"2022-02-18T15:49:59.200319Z","iopub.status.idle":"2022-02-18T15:52:32.951403Z","shell.execute_reply.started":"2022-02-18T15:49:59.200293Z","shell.execute_reply":"2022-02-18T15:52:32.950522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time lda_analysis(w2v.LineSentence('../input/csci-270-tokenized-books-2022/The Martian - Andy Weir_fixed.txt'))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:52:32.952868Z","iopub.execute_input":"2022-02-18T15:52:32.953169Z","iopub.status.idle":"2022-02-18T15:52:33.042291Z","shell.execute_reply.started":"2022-02-18T15:52:32.953127Z","shell.execute_reply":"2022-02-18T15:52:33.041153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis of Additional Books\n\nFor each of the additional books, answer the following questions:\n\n1. How do the topic terms that are not book-specific compare to those from your own book? \n\n**I can still see how the terms that are more generic still apply to their own book because they are more fitting for the vocabulary of their time.**\n\n2. What similarities and differences between the books can you identify based on examining the listed terms?\n\n**Well the two books I choose are drastically different. Without using any of my prior knowledge of these books, I can tell that the martian deals with more material subjects, and the bible deals with more spiritual subjects.\nMartian: hydrogen, water, solar, mars, rover, hydrogen, regulator, cells, trailer\nBible:  thee, thy, thou, faith, spirit, holy**","metadata":{}},{"cell_type":"markdown","source":"# Word2Vec\n\nWord2Vec is a machine learning algorithm that finds similarities between words based on how they are used in documents within a particular corpus. We will begin by building a Word2Vec model of your book. We will then compare the results with building a Word2Vec model of the entire corpus.","metadata":{}},{"cell_type":"code","source":"model = w2v.Word2Vec(sentences)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:54:39.660916Z","iopub.execute_input":"2022-02-18T15:54:39.661204Z","iopub.status.idle":"2022-02-18T15:54:40.373145Z","shell.execute_reply.started":"2022-02-18T15:54:39.661176Z","shell.execute_reply":"2022-02-18T15:54:40.372298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_similarities(model, test_word_list):\n    for test_word in test_word_list:\n        if test_word in model.wv.key_to_index:\n            print(f\"Words similar to {test_word}\")\n            print(model.wv.most_similar(test_word, topn=20))\n            print()\n        else:\n            print(f\"{test_word} is not present in word2vec corpus\")\n            \n    headers = ['Word'] + test_word_list\n    rows = [[word2] + [model.wv.similarity(word1, word2) for word1 in test_word_list] for word2 in test_word_list]\n    show_markdown_table(headers, rows)\n            \n    # Graphing code from: https://www.askpython.com/python-modules/gensim-word2vec\n    X = model.wv[test_word_list]\n    pca = PCA(n_components=2)\n    result = pca.fit_transform(X)\n\n    plt.scatter(result[:, 0], result[:, 1])\n    for i, word in enumerate(test_word_list):\n        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:54:44.581696Z","iopub.execute_input":"2022-02-18T15:54:44.582Z","iopub.status.idle":"2022-02-18T15:54:44.593032Z","shell.execute_reply.started":"2022-02-18T15:54:44.581968Z","shell.execute_reply":"2022-02-18T15:54:44.591897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Terms to analyze\n\nSelect ten terms from your answers to Questions 3 and 4 in the **Analyzing Topics** section above:\n\n**Tom, huck, jim, king, duke, father, river, raft,  lie, steal**\n\n\nExplain why you selected each of these terms:\n\n**I selected the first 6 terms because they are all characters. The next two are places and objects that commonly appear in the book. The next two are just actions that I think commonly occur in huck finn.**\n\nCreate a variable `term_list` below as a list of your ten selected terms.","metadata":{}},{"cell_type":"code","source":"# Place your ten terms in this list\nterm_list = ['tom', 'huck', 'jim', 'king', 'duke', 'father', 'river', 'raft', 'lie', 'steal']","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:05.91258Z","iopub.execute_input":"2022-02-18T15:55:05.912855Z","iopub.status.idle":"2022-02-18T15:55:05.917663Z","shell.execute_reply.started":"2022-02-18T15:55:05.912827Z","shell.execute_reply":"2022-02-18T15:55:05.916742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Term Analysis 1\n\nRun the `word_similarities()` function with your terms in `term_list`. Then answer the questions that follow.","metadata":{}},{"cell_type":"code","source":"word_similarities(model, term_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:12.389748Z","iopub.execute_input":"2022-02-18T15:55:12.390213Z","iopub.status.idle":"2022-02-18T15:55:12.647518Z","shell.execute_reply.started":"2022-02-18T15:55:12.390161Z","shell.execute_reply":"2022-02-18T15:55:12.645634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Term Analysis 1 Questions\n\n1. For each term, what similar terms are most pertinent? \n**Tom: jim is the most similiar. Tom does not appear much in the story but it is usually with jim.\nhuck: jim is not the most similia but is included in the list. It makes sense because they spend majority of the story together.\njim: the only word similiar to jim that has some discernable meaning is raft, because it is the setting of most of the story.\nking, duke: both of these characters have some similarity to jim, which makes sense in the context of them all being runaways.\nfather: the only term that seems to have some relation to the story is dollars. huck's father only comes into the story when he finds out about huck's fortune.\nriver: raft, jim, water, run.\nraft: run, jim, town.\nlie: men, man, young, people.\nsteal: take is the only similiar word that is comparable to steal. I find it interesting that the only character in this list is jim, because huck certainly does majority if not all of the stealing**\n\n2. Which similar terms are least pertinent?\n**For the most part the similiar terms contain generic terms that are changed a little because of the dialect spoken in the text:\nknowed, warnt, id, em, reckoned.**\n\n3. What insights about the structure of the book are represented by the pertinent similar terms?\n**These terms show some relation between the characters, places and actions that take place in the story. But i feel as though these terms do not represent the most important relationships that appear in the text. At least it does not match my assumptions of what it would be.**","metadata":{}},{"cell_type":"markdown","source":"## Term Analysis 2\n\nRepeat the above analysis using a Word2Vec model of the entire corpus.","metadata":{}},{"cell_type":"code","source":"sentences = w2v.PathLineSentences('/kaggle/input/csci-270-tokenized-books-2022', max_sentence_length=5000)\nmodel = w2v.Word2Vec(sentences)\nword_similarities(model, term_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T16:21:11.856125Z","iopub.execute_input":"2022-02-18T16:21:11.856488Z","iopub.status.idle":"2022-02-18T16:21:24.464985Z","shell.execute_reply.started":"2022-02-18T16:21:11.85645Z","shell.execute_reply":"2022-02-18T16:21:24.464187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Term Analysis 2 Questions\n\n1. What did you find to be the most striking differences in the lists of similar terms?\n\n**The most striking difference is that most of the similarity values are lower than they were when analyzing the book alone. The next most aparent difference is that the characters share distinct similarities with characters from other books.**\n\n2. Overall, is it more beneficial to train Word2Vec on your book alone or on the whole corpus? Support your answer with specific details as to how the content of your book interacts with the similarity lists Word2Vec generated.\n\n**I feel as though it is significantly better to train word2vec over a whole corpus. In my experience the terms and topics generated from my book alone were overwelmingly generic and bland. When analyzing the whole corpus there is at least more data to compare to the specifc terms from your book, even it it takes a bit of research to determine relationships.**","metadata":{}},{"cell_type":"markdown","source":"## Concluding Analysis\n\n1. In what ways did you find LDA useful in summarizing aspects of your book?\n\n**I feel as though LDA did not do a sufficient job determining topics for my book. This is probably because of the way huck-finn is tole. I imagine that others had an easier time determining meaning drom**\n\n2. In what ways did you find Word2Vec useful in finding and documenting connections among ideas, concepts, and characters in your book?\n\n**I did not find it useful in matching my intuition of the relationships between characters settings and actions. But it did find some similairities that i did not expect. Overall I think that word2vec did a fair job, considering how much data it is processing.**\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}